<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization">
  <meta property="og:title" content="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization"/>
  <meta property="og:description" content="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization"/>
  <meta property="og:url" content="https://research.nvidia.com/labs/lpr/gdpo_rl/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization">
  <meta name="twitter:description" content="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>GDPO: Group reward-Decoupled Normalization
Policy Optimization for Multi-reward RL
Optimization</title>
  <link rel="icon" type="image/x-icon" href="static/images/gdpo_brain.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://d3js.org/d3.v7.min.js"></script>
  
  <!-- MathJax for LaTeX rendering -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script src="https://unpkg.com/d3-sankey@0.12.3/dist/d3-sankey.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="document_samples.js"></script>
<!-- OneTrust Cookies Consent Notice start for nvidia.com -->
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="3e2b62ff-7ae7-4ac5-87c8-d5949ecafff5" ></script>
<script type="text/javascript">
function OptanonWrapper() {       
        var event = new Event('bannerLoaded');
        window.dispatchEvent(event);
    }
</script>
<!-- OneTrust Cookies Consent Notice end for nvidia.com -->
<script type="text/javascript" src="https://images.nvidia.com/aem-dam/Solutions/ot-js/ot-custom.js"></script>

<style>
  .policy-links {
    margin-top: 2rem;
    font-size: 0.9rem;
    line-height: 1.8;
  }
  
  .policy-link {
    color: #4a4a4a;
    padding: 0.2rem 0.5rem;
    transition: color 0.3s, background-color 0.3s;
    border-radius: 4px;
  }
  
  .policy-link:hover {
    color: #3273dc;
    background-color: #f5f5f5;
    text-decoration: none;
  }
  
  .separator {
    margin: 0 0.2rem;
    color: #dbdbdb;
  }
  
  @media screen and (max-width: 768px) {
    .policy-links {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
    }
    
    .policy-link {
      margin: 0.3rem;
    }
    
    .separator {
      display: none;
    }
  }
  
  /* Add these custom styles to your existing styles */
  .container.is-fullwidth {
    max-width: 100%;
    width: 100%;
    padding: 0 1rem;
  }
  
  .content figure.image img.large-diagram {
    width: 100%;
    max-height: 500px;
    object-fit: contain;
  }
  
  .results-table-container {
    overflow-x: auto;
    max-width: 100%;
    width: 100%;
    margin: 0;
    padding: 0;
  }
  
  .table-wrapper {
    padding: 0;
  }
  
  .compact-table {
    font-size: 0.75rem !important;
    width: 100%;
  }
  
  .compact-table th, 
  .compact-table td {
    padding: 0.4em 0.5em !important;
    white-space: nowrap;
  }

  .compact-table strong {
    font-weight: 700;
    color: #209cee;
  }
  
  @media screen and (max-width: 1023px) {
    .results-table-container {
      margin-left: -1rem;
      margin-right: -1rem;
      width: calc(100% + 2rem);
    }
  }
  
  @media screen and (min-width: 1024px) {
    .container.is-fullwidth {
      padding: 0 2rem;
    }
  }

  /* Extend the width of the Pre-training from Scratch section too */
  .container.is-wider {
    max-width: 1344px;
    margin: 0 auto;
  }

  /* ‰øÆÊîπÂõæÁâáËØ¥ÊòéÊñáÂ≠óÊ†∑ÂºèÔºöÂçïË°åÂ±Ö‰∏≠ÔºåÂ§öË°åÂ∑¶ÂØπÈΩê */
  figure.image figcaption {
    margin-top: 0.75rem;
    line-height: 1.4;
    color: #4a4a4a;
    font-size: 0.9rem;
    padding: 0 0.5rem;
    text-align: left; /* ÈªòËÆ§Â∑¶ÂØπÈΩê */
    max-width: 100%;
  }
  
  /* ÂçïË°åÊñáÊú¨Â±Ö‰∏≠ÁöÑÊäÄÂ∑ß */
  figure.image figcaption:not(:has(br)):not([style*="height"])[style*="display: -webkit-box;"] {
    text-align: center;
  }
  
  /* ÁÆÄÂçïÊõø‰ª£ÊñπÊ°àÔºöÊ£ÄÊµãÁü≠ÊñáÊú¨Âπ∂Â±Ö‰∏≠ */
  figure.image figcaption:not(:has(br)):not([style*="height"]) {
    text-align: center;
  }
  
  /* Ë°®Ê†ºÊ†áÈ¢ò‰ΩøÁî®Áõ∏ÂêåÁöÑÊ†∑ÂºèÈÄªËæë */
  table caption {
    margin-bottom: 1rem;
    color: #4a4a4a;
    font-weight: bold;
    text-align: left;
  }
  
  /* ÂçïË°åË°®Ê†ºÊ†áÈ¢òÂ±Ö‰∏≠ */
  table caption:not(:has(br)):not([style*="height"]) {
    text-align: center;
  }
</style>
  
<script>
  /* ‰∏∫Â§ÑÁêÜ‰∏çÂêåÊµèËßàÂô®ÂÖºÂÆπÊÄßÁöÑJavaScriptÂáΩÊï∞ */
  window.addEventListener('DOMContentLoaded', function() {
    // Â§ÑÁêÜÂõæÁâáËØ¥Êòé
    document.querySelectorAll('figure.image figcaption').forEach(function(caption) {
      // Â¶ÇÊûúÊñáÊú¨Ê≤°ÊúâÊç¢Ë°å‰∏îÂÆΩÂ∫¶‰∏çË∂ÖËøá‰∏ÄÂÆöÊØî‰æãÔºåËÆ§‰∏∫ÊòØÂçïË°åÊñáÊú¨
      if (caption.offsetHeight < 24 && caption.textContent.length < 100) {
        caption.style.textAlign = 'center';
      } else {
        caption.style.textAlign = 'left';
      }
    });
    
    // Â§ÑÁêÜË°®Ê†ºÊ†áÈ¢ò
    document.querySelectorAll('table caption').forEach(function(caption) {
      if (caption.offsetHeight < 24 && caption.textContent.length < 100) {
        caption.style.textAlign = 'center';
      } else {
        caption.style.textAlign = 'left';
      }
    });
  });
</script>

</head>
<body>

  <section class="hero" style="margin-bottom: -30px;">
    <div class="hero-body" style="padding: 1rem 1.5rem;">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="margin-bottom: 0.5rem;">
              <img src="./static/images/gdpo_brain.png" alt="DLER" style="height: 1.2em; vertical-align: middle; margin-right: 0.3em; display: inline-block;">
              GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization
            </h1>
            <div class="is-size-5 publication-authors" style="margin-bottom: 0.5rem;">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://nbasyl.github.io/" target="_blank">Shih-Yang Liu<sup>1</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://simonxin.com/" target="_blank">Xin Dong<sup>2</sup></a>,</span>
                    <span class="author-block">
                      <a href="https://gloriaximinglu.github.io/" target="_blank">Ximing Lu</a>,</span>
                      <span class="author-block">
                        <a href="https://shizhediao.github.io/" target="_blank">Shizhe Diao</a>,</span>
                    <span class="author-block">
                      <span class="author-block">
                        <a href="https://pbelcak.com/" target="_blank">Peter Belcak</a>,</span>
                    <span class="author-block">
                      <a href="https://research.nvidia.com/person/mingjie-liu" target="_blank"> Mingjie Liu</a>,</span>
                      <span class="author-block">
                        <a href="https://minhungchen.netlify.app/" target="_blank">Min-Hung Chen</a>,</span>
                        <span class="author-block">
                          <a href="https://hongxu-yin.github.io/" target="_blank">Hongxu Yin</a>,</span>
                          <span class="author-block">
                            <a href="https://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>,</span>
                            <span class="author-block">
                              <a href="https://seng.hkust.edu.hk/about/people/faculty/tim-kwang-ting-cheng" target="_blank">Kwang-Ting Cheng<sup>1</sup></a>,</span>
                              <span class="author-block">
                                <a href="https://yejinc.github.io/" target="_blank">Yejin Choi</a>,</span>
                                <span class="author-block">
                                          <span class="author-block">
                                            <a href="https://jankautz.com/" target="_blank">Jan Kautz</a>,</span>                                
                                              <span class="author-block">
                                                <a href="https://www.pmolchanov.com/" target="_blank">Pavlo Molchanov</a></span>

                </div>

                  <div class="is-size-5 publication-authors" style="margin-bottom: 0.5rem;">
                    <span class="author-block">
                      <img src="https://www.nvidia.com/favicon.ico" alt="NVIDIA" style="width: 2em; height: 2em; vertical-align: middle; margin-right: 0.3em;">
                      NVIDIA
                    </span>
                    <span class="eql-cntrb"><small><br><sup>1</sup> affiliated with HKUST. Work done during Shih-Yang‚Äôs internship at NVIDIA <sup>2</sup> project lead</small></span>
                  </div>

                  <div class="column has-text-centered" style="margin-bottom: 0;">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon" style="font-size:18px">ü§ó</span>
                        <span>HF Paper page</span>
                      </a>
                    </span>
                    <!-- <span class="link-block">
                        <a href="https://huggingface.co/datasets/nvidia/ClimbMix" class="external-link button is-normal is-rounded is-dark">
                          <span class="icon" style="font-size:18px"></span>
                          <span>Code</span>
                        </a>
                      </span> -->
                      <span class="link-block">
                        <a href="https://github.com/NVlabs/GDPO" target="_blank" class="external-link button is-normal is-rounded is-dark">
                          <span class="icon" style="font-size:18px">
                            <i class="fab fa-github"></i>
                          </span>
                          <span>Code</span>
                        </a>
                      </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>






<!-- Paper abstract -->
<section class="section hero is-light" style="padding: 1rem 1.5rem; margin-top: 0px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-bottom: 0.75rem;">What's wrong with using GRPO for multi-reward RL training?</h2>
        <div class="content has-text-justified">
          <ul>
            <li>
              We present <strong>Group Reward‚ÄìDecoupled Normalization Policy Optimization (GDPO)</strong>, a drop-in replacement for GRPO that consistently delivers better convergence on each reward for multi-reward RL training.
            </li>
            <li>
              We find that the common practice of applying GRPO to multi-reward RL optimization leads to a previously overlooked issue: GRPO inherently collapses reward signals, resulting in information loss in the advantage estimates.
            </li>
          </ul>
        </div>
          <figure class="image">
            <img src="static/images/gdpo_toy.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 1: Comparison of GRPO and GDPO advantage computation in a two-binary-reward, two-rollout
                  example. GRPO maps different reward combinations into only two distinct advantage groups, whereas
                  GDPO normalizes each reward independently and retains three distinct groups of advantage values. We skip
                  the batch-wise normalization calculation step in GDPO here for simplicity since it does not change the number
                  of distinct advantage groups.</figcaption>
          </figure>
          <!-- add some space here -->
          <br>
          <figure style="position: relative; left: 50%; transform: translateX(-50%); width: 120%; text-align:center;">
            <div style="
              display: flex;
              justify-content: center;
              align-items: flex-start;
              gap: 24px;
              flex-wrap: wrap;
            ">
              <figure style="text-align: center; flex: 1 1 48%; max-width: 48%;">
                <img src="static/images/plot_TRI_7B.png" alt="SFT-trained model" style="width: 100%; height: auto; object-fit: contain;">
                <figcaption style="margin-top: 6px;">(a)</figcaption>
              </figure>
          
              <figure style="text-align: center; flex: 1 1 48%; max-width: 48%;">
                <img src="static/images/latency_7b_1.png" alt="RL-trained model" style="width: 100%; height: auto; object-fit: contain;">
                <figcaption style="margin-top: 6px;">(b)</figcaption>
              </figure>
            </div>
          
            <figcaption style="text-align: center; margin-top: 10px;">
              Figure 2: (a) DLER achieves state-of-the-art accuracy/length trade-offs, shortening CoT by up to 70% without losing accuracy. (b) On AIME-24, DLER-R1 models enable better test-time scaling.
            </figcaption>
          </figure>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Overview Section -->
<section class="section" style="padding-top: 1rem; padding-bottom: 1rem;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3" style="margin-bottom: 0.75rem;">What goes wrong when applying a length penalty? Re-examining the Simplest Length Penalty - Truncation</h2>
        <div class="content">
          <h3 class="title is-4" style="margin-bottom: 0.75rem; color:forestgreen;">Stricter Length Penalty Leads to Higher Reward Variance</h3></li>
          <div class="content has-text-justified mt-4">
            <p>
              More aggressive truncation introduces greater training instability by increasing group-wise advantage variance, which in turn leads to more biased advantage estimates, thus we propose to swap out the group-wise normalization with batch-wise normalization to mitigate this issue.
            </p>
          </div>
          <!-- <figure class="image">
            <img src="static/images/plot_grpo_reinforce_horizontal.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 1: Accuracy and average response length of DeepSeek-R1-7B on the AIME-24 test set, evaluated every
              10 training steps across two RL training runs: group-wise reward normalization (GRPO) and batch-wise
              normalization. GRPO shows declining accuracy while batch-wise reward normalization remains stable despite
              reduced token counts.</figcaption>
          </figure> -->
        </div>
        <div class="content">
          <h3 class="title is-4" style="margin-bottom: 0.75rem; color:forestgreen;">Entropy Collapse Limits Exploration of Reasoning Paths</h3></li>
          <div class="content has-text-justified mt-4">
            <p>
              Clipping the updates of low-probability, high-entropy tokens‚Äîessential for exploring diverse reasoning paths‚Äîcan cause an entropy collapse that limits exploration. Increasing the clipping threshold preserves these tokens during gradient updates, thereby alleviating entropy collapse.
            </p>
          </div>
          <figure class="image">
            <img src="static/images/entropy.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 3: Left: Word clouds of the most frequent tokens clipped by the high-threshold before applying higher clipping threshold,showing that many are transitional words important for reasoning, and clipping them limits exploration during RL training. (Right) Average probability and entropy of tokens not clipped, clipped by the higher threshold, and clipped by the lower threshold during RL training. Clipped tokens have much lower probabilities than unclipped ones, and those clipped by the higher threshold consistently show higher entropy, supporting that these are often high-entropy transitional tokens triggering reasoning steps.</figcaption>
          </figure>
        </div>
        <div class="content">
          <h3 class="title is-4" style="margin-bottom: 0.75rem; color:forestgreen;">Length Penalty Over-sparsify Training Signal</h3></li>
          <div class="content has-text-justified mt-4">
            <p>
              Applying length penalty makes early training too difficult and later stages dominated by easy samples. We adopt Dynamic Sampling, which filters out overly easy/hard prompts and extreme response lengths, yielding balanced training signals and better length control.
            </p>
          </div>
          <figure class="image">
            <img src="static/images/ds_filter_combined.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 4: Left: Ratio of training prompts with all 16 rollouts receiving zero reward, including those caused by exceeding the truncation length. Around half of the prompts fall into this category early in training, weakening the signal and biasing the model toward easier prompts that model already know how to solve within the target length. Right: Ratio of training prompts with all 16 rollouts receiving reward score of one steadily increases, while average response length declines and remains markedly shorter than that for prompts whose rollouts all receive a reward of zero.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-fullwidth">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-2">Combining All Ingredients: Do Length pEnalty Right (DLER)</h2>

        <div class="content has-text-justified mt-4">
            <p style="color: forestgreen;">
              We unify batch-wise reward normalization, a higher policy update clipping
              threshold, dynamic sampling to remove instances lacking balanced training signals, and a simple length
              truncation penalty into a comprehensive training recipe, which we term DLER (Doing Length pEnalty
              Right).
            </p>
        </div>
        <h5 class="title is-5">Training Code will be released soon!</h5>

      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-wider">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">State of the Art Accuracy/Response Length Trade-offs</h2>
        <div class="content">
          <div class="columns is-centered">
            <div class="column is-12">
              <figure class="image" style="margin-top: 24px;">
                <div class="columns is-centered">
                  <!-- <div style="display: flex; justify-content: center; width: 100%;"> -->
                    <figure style="
                    position: relative;
                    left: 50%;
                    transform: translateX(-50%);
                    width: 140%;
                    margin-top: 0;
                    margin-bottom: 1em;
                    text-align: center;
                  ">
                    <div style="display: inline-block; width: 100%; text-align: center;">
                      <img src="static/images/table.png" alt="Data Mixture Comparison" style="
                        width: 100%; height: auto; object-fit: contain;
                        border: 1px solid #eaeaea; border-radius: 8px;
                        box-shadow: 0 4px 8px rgba(0,0,0,0.05);
                        background-color: white; padding: 15px;
                      ">
                      <figcaption style="
                        display: block;
                        width: 100%;
                        margin-top: 12px;
                        text-align: center;
                        font-size: 0.95rem;
                      ">
                        Table 1: Comparison of DLER models and baseline models in terms of Pass@1 accuracy and corresponding
                        average output length (tokens) across benchmarks.
                      </figcaption>
                    </div>
                  </figure>
                  <!-- </div> -->

                </div>
              </figure>
            </div>
          </div>
        </div>
        <div class="content has-text-justified mt-4">
            <p>
                <!-- Based on the insights obtained from our explorations, we apply CLIMB to two existing datasets: Nemotron-CC and smollm-corpus, with the goal of constructing a powerful new pre-training dataset. 
                Specifically, we first combine Nemotron-CC and smollm-corpus, and then employ our proposed CLIMB-clustering method to semantically reorganize and filter this combined dataset into 20 distinct clusters, leading to a 1.2-trillion-token high-quality corpus, named <span style="color:#e15043; font-weight:500;">ClimbLab</span>. 
                Subsequently, we utilize CLIMB-search to identify an optimal data mixture from these clusters.
                Using this optimal mixture, we further extract a 400-billion-token high-quality dataset named <span style="color: #8046a1; font-weight:500;">ClimbMix</span>.
                We publicly release these two datasets: the filtered 1.2-trillion-token dataset organized into 20 semantic clusters as a research playground for further data-mixture studies, and the optimized 400-billion-token ClimbMix dataset for efficient pre-training. -->
            </p>
            <p>
                <!-- We train a 1B model from scratch with ClimbMix and evaluate its performance relative to models pretrained on other datasets under the same token budget. 
                The results in Figure 3 indicate that models trained on ClimbMix significantly outperform those trained on existing datasets.  -->
            </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-wider">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Different Length Penalties No Longer Push the Accuracy‚ÄìEfficiency Frontier</h2>

        <div class="content has-text-justified mt-4">
            <p>
              We show that with DLER, the effect of adopting different
              length-penalty rewards fundamentally changes. Specifically, the accuracy‚Äìlength relationship is no longer
              altered in a way that yields strictly shorter responses with higher accuracy; instead, a trade-off always exists.
            </p>
        </div>
        <figure style="position: relative; left: 50%; transform: translateX(-50%); width: 100%; text-align:center; margin: 0;">
          <div style="
            display: flex;
            justify-content: center;
            align-items: stretch;          /* make both columns equal height */
            gap: 24px;
            flex-wrap: nowrap;             /* keep them on one line */
            box-sizing: border-box;
          ">
            <!-- Subfigure A -->
            <figure style="
              margin: 0;                   /* remove default figure margins */
              text-align: center;
              flex: 0 0 calc(50% - 12px);  /* 50% minus half the gap */
              max-width: calc(50% - 12px);
              display: flex;               /* stack image and caption */
              flex-direction: column;
            ">
              <img src="static/images/diff_length.png" alt="MATH"
                   style="width: 100%; height: auto; display: block; object-fit: contain;">
              <figcaption style="margin-top: 6px;">(a) MATH</figcaption>
            </figure>
        
            <!-- Subfigure B -->
            <figure style="
              margin: 0;
              text-align: center;
              flex: 0 0 calc(50% - 12px);
              max-width: calc(50% - 12px);
              display: flex;
              flex-direction: column;
            ">
              <img src="static/images/diff_length_aime.png" alt="AIME-24"
                   style="width: 100%; height: auto; display: block; object-fit: contain;">
              <figcaption style="margin-top: 6px;">(b) AIME-24</figcaption>
            </figure>
          </div>
        
          <figcaption style="text-align: center; margin-top: 10px;">
            Figure 5: Accuracy and average response length of DeepSeek-R1-7B trained using DLER with different length
            penalties on MATH and AIME-24. DLER establishes a new accuracy‚Äìlength efficiency frontier,
            with varying length penalties moving performance along the frontier rather than beyond it.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-wider">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Overcoming Quality Limitations of Publicly Available Training Data</h2>
        <div class="content">
          <div class="columns is-centered">
            <div class="column is-12">
              <figure class="image" style="margin-top: 24px;">
                <div class="columns is-centered">
                    <div class="column is-7">
                        <figure class="image" style="margin-top: 0px;">
                          <img src="static/images/final_data_comparison.png" alt="Data Mixture Comparison" class="large-diagram" 
                            style="max-height: 480px; width: 100%; object-fit: contain; 
                            border: 1px solid #eaeaea; border-radius: 8px; 
                            box-shadow: 0 4px 8px rgba(0,0,0,0.05); 
                            background-color: white; padding: 15px;">
                          <figcaption style="text-align: left; margin-top: 15px; font-size: 0.95rem;">
                            Figure 3: Training a 1B model from scratch on ClimbMix shows better scaling effects than training on other datasets.
                          </figcaption>
                        </figure>
                      </div>

                </div>
              </figure>
            </div>
          </div>
        </div>
        <div class="content has-text-justified mt-4">
            <p>
                Based on the insights obtained from our explorations, we apply CLIMB to two existing datasets: Nemotron-CC and smollm-corpus, with the goal of constructing a powerful new pre-training dataset. 
                Specifically, we first combine Nemotron-CC and smollm-corpus, and then employ our proposed CLIMB-clustering method to semantically reorganize and filter this combined dataset into 20 distinct clusters, leading to a 1.2-trillion-token high-quality corpus, named <span style="color:#e15043; font-weight:500;">ClimbLab</span>. 
                Subsequently, we utilize CLIMB-search to identify an optimal data mixture from these clusters.
                Using this optimal mixture, we further extract a 400-billion-token high-quality dataset named <span style="color: #8046a1; font-weight:500;">ClimbMix</span>.
                We publicly release these two datasets: the filtered 1.2-trillion-token dataset organized into 20 semantic clusters as a research playground for further data-mixture studies, and the optimized 400-billion-token ClimbMix dataset for efficient pre-training.
            </p>
            <p>
                We train a 1B model from scratch with ClimbMix and evaluate its performance relative to models pretrained on other datasets under the same token budget. 
                The results in Figure 3 indicate that models trained on ClimbMix significantly outperform those trained on existing datasets. 
            </p>
        </div>
      </div>

    </div>
  </div>
</section> -->

<!-- Cluster Table Section -->

<!-- End Cluster Table Section -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{liu2025dlerdoinglengthpenalty,
          title={DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning}, 
          author={Shih-Yang Liu and Xin Dong and Ximing Lu and Shizhe Diao and Mingjie Liu and Min-Hung Chen and Hongxu Yin and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Yejin Choi and Jan Kautz and Pavlo Molchanov},
          year={2025},
          eprint={2510.15110},
          archivePrefix={arXiv},
          primaryClass={cs.LG},
          url={https://arxiv.org/abs/2510.15110}, 
    }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

          <div class="policy-links has-text-centered">
            <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank" rel="noopener" class="policy-link">Privacy Policy</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/privacy-center/" target="_blank" rel="noopener" class="policy-link">Manage My Privacy</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/preferences/email-preferences/" target="_blank" rel="noopener" class="policy-link">Do Not Sell or Share My Data</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank" rel="noopener" class="policy-link">Terms of Service</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank" rel="noopener" class="policy-link">Accessibility</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank" rel="noopener" class="policy-link">Corporate Policies</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/contact/" target="_blank" rel="noopener" class="policy-link">Contact</a>
          </div>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
