<!DOCTYPE html>
<html>
<head>

  

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization">
  <meta property="og:title" content="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization"/>
  <meta property="og:description" content="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization"/>
  <meta property="og:url" content="https://research.nvidia.com/labs/lpr/gdpo_rl/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization">
  <meta name="twitter:description" content="GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>GDPO: Group reward-Decoupled Normalization
Policy Optimization for Multi-reward RL
Optimization</title>
  <link rel="icon" type="image/x-icon" href="static/images/gdpo_brain.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- MathJax for LaTeX rendering -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script src="https://unpkg.com/d3-sankey@0.12.3/dist/d3-sankey.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="document_samples.js"></script>
<!-- OneTrust Cookies Consent Notice start for nvidia.com -->
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="3e2b62ff-7ae7-4ac5-87c8-d5949ecafff5" ></script>
<script type="text/javascript">
function OptanonWrapper() {       
        var event = new Event('bannerLoaded');
        window.dispatchEvent(event);
    }
</script>
<!-- OneTrust Cookies Consent Notice end for nvidia.com -->
<script type="text/javascript" src="https://images.nvidia.com/aem-dam/Solutions/ot-js/ot-custom.js"></script>

<style>
  .policy-links {
    margin-top: 2rem;
    font-size: 0.9rem;
    line-height: 1.8;
  }
  
  .policy-link {
    color: #4a4a4a;
    padding: 0.2rem 0.5rem;
    transition: color 0.3s, background-color 0.3s;
    border-radius: 4px;
  }
  
  .policy-link:hover {
    color: #3273dc;
    background-color: #f5f5f5;
    text-decoration: none;
  }
  
  .separator {
    margin: 0 0.2rem;
    color: #dbdbdb;
  }
  
  @media screen and (max-width: 768px) {
    .policy-links {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
    }
    
    .policy-link {
      margin: 0.3rem;
    }
    
    .separator {
      display: none;
    }
  }
  
  /* Add these custom styles to your existing styles */
  .container.is-fullwidth {
    max-width: 100%;
    width: 100%;
    padding: 0 1rem;
  }
  
  .content figure.image img.large-diagram {
    width: 100%;
    max-height: 500px;
    object-fit: contain;
  }
  
  .results-table-container {
    overflow-x: auto;
    max-width: 100%;
    width: 100%;
    margin: 0;
    padding: 0;
  }
  
  .table-wrapper {
    padding: 0;
  }
  
  .compact-table {
    font-size: 0.75rem !important;
    width: 100%;
  }
  
  .compact-table th, 
  .compact-table td {
    padding: 0.4em 0.5em !important;
    white-space: nowrap;
  }

  .compact-table strong {
    font-weight: 700;
    color: #209cee;
  }
  
  @media screen and (max-width: 1023px) {
    .results-table-container {
      margin-left: -1rem;
      margin-right: -1rem;
      width: calc(100% + 2rem);
    }
  }
  
  @media screen and (min-width: 1024px) {
    .container.is-fullwidth {
      padding: 0 2rem;
    }
  }

  /* Extend the width of the Pre-training from Scratch section too */
  .container.is-wider {
    max-width: 1344px;
    margin: 0 auto;
  }

  /* ‰øÆÊîπÂõæÁâáËØ¥ÊòéÊñáÂ≠óÊ†∑ÂºèÔºöÂçïË°åÂ±Ö‰∏≠ÔºåÂ§öË°åÂ∑¶ÂØπÈΩê */
  figure.image figcaption {
    margin-top: 0.75rem;
    line-height: 1.4;
    color: #4a4a4a;
    font-size: 0.9rem;
    padding: 0 0.5rem;
    text-align: left; /* ÈªòËÆ§Â∑¶ÂØπÈΩê */
    max-width: 100%;
  }
  
  /* ÂçïË°åÊñáÊú¨Â±Ö‰∏≠ÁöÑÊäÄÂ∑ß */
  figure.image figcaption:not(:has(br)):not([style*="height"])[style*="display: -webkit-box;"] {
    text-align: center;
  }
  
  /* ÁÆÄÂçïÊõø‰ª£ÊñπÊ°àÔºöÊ£ÄÊµãÁü≠ÊñáÊú¨Âπ∂Â±Ö‰∏≠ */
  figure.image figcaption:not(:has(br)):not([style*="height"]) {
    text-align: center;
  }
  
  /* Ë°®Ê†ºÊ†áÈ¢ò‰ΩøÁî®Áõ∏ÂêåÁöÑÊ†∑ÂºèÈÄªËæë */
  table caption {
    margin-bottom: 1rem;
    color: #4a4a4a;
    font-weight: bold;
    text-align: left;
  }
  
  /* ÂçïË°åË°®Ê†ºÊ†áÈ¢òÂ±Ö‰∏≠ */
  table caption:not(:has(br)):not([style*="height"]) {
    text-align: center;
  }
</style>
  
<script>
  /* ‰∏∫Â§ÑÁêÜ‰∏çÂêåÊµèËßàÂô®ÂÖºÂÆπÊÄßÁöÑJavaScriptÂáΩÊï∞ */
  window.addEventListener('DOMContentLoaded', function() {
    // Â§ÑÁêÜÂõæÁâáËØ¥Êòé
    document.querySelectorAll('figure.image figcaption').forEach(function(caption) {
      // Â¶ÇÊûúÊñáÊú¨Ê≤°ÊúâÊç¢Ë°å‰∏îÂÆΩÂ∫¶‰∏çË∂ÖËøá‰∏ÄÂÆöÊØî‰æãÔºåËÆ§‰∏∫ÊòØÂçïË°åÊñáÊú¨
      if (caption.offsetHeight < 24 && caption.textContent.length < 100) {
        caption.style.textAlign = 'center';
      } else {
        caption.style.textAlign = 'left';
      }
    });
    
    // Â§ÑÁêÜË°®Ê†ºÊ†áÈ¢ò
    document.querySelectorAll('table caption').forEach(function(caption) {
      if (caption.offsetHeight < 24 && caption.textContent.length < 100) {
        caption.style.textAlign = 'center';
      } else {
        caption.style.textAlign = 'left';
      }
    });
  });
</script>

</head>
<body>

  <section class="hero" style="margin-bottom: -10px;">
    <div class="hero-body" style="padding: 1rem 1.5rem;">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="margin-bottom: 0.5rem;">
              <img src="./static/images/gdpo_brain.png" alt="DLER" style="height: 1.2em; vertical-align: middle; margin-right: 0.3em; display: inline-block;">
              GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization
            </h1>
            <div class="is-size-5 publication-authors" style="margin-bottom: 0.5rem;">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://nbasyl.github.io/" target="_blank">Shih-Yang Liu<sup>1</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://simonxin.com/" target="_blank">Xin Dong<sup>2</sup></a>,</span>
                    <span class="author-block">
                      <a href="https://gloriaximinglu.github.io/" target="_blank">Ximing Lu</a>,</span>
                      <span class="author-block">
                        <a href="https://shizhediao.github.io/" target="_blank">Shizhe Diao</a>,</span>
                    <span class="author-block">
                      <span class="author-block">
                        <a href="https://pbelcak.com/" target="_blank">Peter Belcak</a>,</span>
                    <span class="author-block">
                      <a href="https://research.nvidia.com/person/mingjie-liu" target="_blank"> Mingjie Liu</a>,</span>
                      <span class="author-block">
                        <a href="https://minhungchen.netlify.app/" target="_blank">Min-Hung Chen</a>,</span>
                        <span class="author-block">
                          <a href="https://hongxu-yin.github.io/" target="_blank">Hongxu Yin</a>,</span>
                          <span class="author-block">
                            <a href="https://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a>,</span>
                            <span class="author-block">
                              <a href="https://seng.hkust.edu.hk/about/people/faculty/tim-kwang-ting-cheng" target="_blank">Kwang-Ting Cheng<sup>1</sup></a>,</span>
                              <span class="author-block">
                                <a href="https://yejinc.github.io/" target="_blank">Yejin Choi</a>,</span>
                                <span class="author-block">
                                          <span class="author-block">
                                            <a href="https://jankautz.com/" target="_blank">Jan Kautz</a>,</span>                                
                                              <span class="author-block">
                                                <a href="https://www.pmolchanov.com/" target="_blank">Pavlo Molchanov</a></span>

                </div>

                  <div class="is-size-5 publication-authors" style="margin-bottom: 0.5rem;">
                    <span class="author-block">
                      <img src="https://www.nvidia.com/favicon.ico" alt="NVIDIA" style="width: 2em; height: 2em; vertical-align: middle; margin-right: 0.3em;">
                      NVIDIA
                    </span>
                    <span class="eql-cntrb"><small><br><sup>1</sup> affiliated with HKUST. Work done during Shih-Yang‚Äôs internship at NVIDIA <sup>2</sup> project lead</small></span>
                  </div>

                  <div class="column has-text-centered" style="margin-bottom: 0;">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2601.05242" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon" style="font-size:18px">ü§ó</span>
                        <span>HF Paper page</span>
                      </a>
                    </span>
                    <!-- <span class="link-block">
                        <a href="https://huggingface.co/datasets/nvidia/ClimbMix" class="external-link button is-normal is-rounded is-dark">
                          <span class="icon" style="font-size:18px"></span>
                          <span>Code</span>
                        </a>
                      </span> -->
                      <span class="link-block">
                        <a href="https://github.com/NVlabs/GDPO" target="_blank" class="external-link button is-normal is-rounded is-dark">
                          <span class="icon" style="font-size:18px">
                            <i class="fab fa-github"></i>
                          </span>
                          <span>Code</span>
                        </a>
                      </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section hero is-light" style="padding: 1rem 1.5rem; margin-top: 0px;">
  <br>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-bottom: 0.75rem;">Why Choose GDPO Over GRPO for Multi-Reward RL Training?</h2>
        <div class="content has-text-justified">
          <ul>
            <li>
              We present <strong>Group Reward‚ÄìDecoupled Normalization Policy Optimization (GDPO)</strong>, a drop-in replacement for GRPO that consistently improves per-reward convergence in multi-reward RL training across tasks.
            </li>
          </ul>
        </div>
          <figure class="image">
            <img src="static/images/tool_gdpo.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 1: Reward curves of training Qwen2.5-1.5B-Instruct using GDPO and GRPO on the tool-calling task. GDPO consistently converges to higher correctness and format rewards.</figcaption>
          </figure>
          <!-- add some space here -->
          <!-- <br> -->
          <!-- <figure class="image">
            <img src="static/images/gsm8k_gdpo.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 2: .</figcaption>
          </figure> -->
          <br>
          <figure class="image">
            <img src="static/images/gdpo_dsr1.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 2: Reward score trends during training of DeepSeek-R1-1.5B to reduce response length using GDPO and GRPO; GDPO converges to higher correctness and length rewards.</figcaption>
          </figure>
          <br>
          <figure class="image">
            <img src="static/images/gsm8k_plot.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 3: Reward curves of training Qwen2.5-1.5B-Instruct using GDPO and GRPO on the math reasoning task to achieve the "aha" moment. GDPO consistently converges to higher correctness, format, and integer rewards.</figcaption>
          </figure>
          <!-- <figure style="position: relative; left: 50%; transform: translateX(-50%); width: 120%; text-align:center;">
            <div style="
              display: flex;
              justify-content: center;
              align-items: flex-start;
              gap: 24px;
              flex-wrap: wrap;
            ">
              <figure style="text-align: center; flex: 1 1 48%; max-width: 48%;">
                <img src="static/images/plot_TRI_7B.png" alt="SFT-trained model" style="width: 100%; height: auto; object-fit: contain;">
                <figcaption style="margin-top: 6px;">(a)</figcaption>
              </figure>
          
              <figure style="text-align: center; flex: 1 1 48%; max-width: 48%;">
                <img src="static/images/latency_7b_1.png" alt="RL-trained model" style="width: 100%; height: auto; object-fit: contain;">
                <figcaption style="margin-top: 6px;">(b)</figcaption>
              </figure>
            </div>
          
            <figcaption style="text-align: center; margin-top: 10px;">
              Figure 2: (a) DLER achieves state-of-the-art accuracy/length trade-offs, shortening CoT by up to 70% without losing accuracy. (b) On AIME-24, DLER-R1 models enable better test-time scaling.
            </figcaption>
          </figure> -->
      </div>
    </div>
  </div>
  <br>
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-bottom: 0.75rem;">What's wrong with using GRPO for multi-reward RL training?</h2>
          <figure class="image">
            <img src="static/images/gdpo_toy.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 4: Comparison of GRPO and GDPO advantage computation in a two-binary-reward, two-rollout
                  example. GRPO maps different reward combinations into only two distinct advantage groups, whereas
                  GDPO normalizes each reward independently and retains three distinct groups of advantage values. We skip
                  the batch-wise normalization calculation step in GDPO here for simplicity since it does not change the number
                  of distinct advantage groups.</figcaption>
          </figure>
        <div class="content has-text-justified">
          <ul>
            <li>
              We find that the common practice of applying GRPO to multi-reward RL optimization leads to a previously overlooked issue: GRPO inherently collapses reward signals, resulting in information loss in the advantage estimates.
            </li>
            <li>
              Let's start with a simple training setting and then extend it to more general cases. Consider a scenario where we generate two rollouts for each question for calculating the group-relative advantage and the task involves two binary reward ùëü1, ùëü2 ‚àà {0, 1}. Consequently, the total
              reward for each rollout can take values from {0, 1, 2}.
            </li>
            <li>
              Then from the figure above, we can see that directly applying GRPO for advantage estimation collapses distinct reward combinations (0, 1), (0, 2), and (1, 2) to identical normalized advantages of (‚àí0 .7071 , 0 .7071)
            </li>
            <li>
              <strong>Our intuition:</strong> We think that such characteristic of GRPO‚Äôs advantage calculation in multi-reward optimization over-compresses the rich group-wise reward signal, reducing the resolution of the training signal and leading to suboptimal convergence.
            </li>
            <li>
              For example, reward combination of (0, 2) should produce a stronger learning
signal than (0, 1) because a total reward of 2 indicates simultaneous satisfaction of rewards ùëü1 and ùëü2, whereas a
reward of 1 corresponds to achieving only ùëü1 or ùëü2. Thus, when the other one rollout only receives zero reward,
(0, 2) should yield a larger relative advantage than (0, 1). This limitation can also introduce risks of training
instability due to inaccurate advantage estimates. 
            </li>
          </ul>
        </div>

          <!-- add some space here -->
          <br>
          <!-- <figure style="position: relative; left: 50%; transform: translateX(-50%); width: 120%; text-align:center;">
            <div style="
              display: flex;
              justify-content: center;
              align-items: flex-start;
              gap: 24px;
              flex-wrap: wrap;
            ">
              <figure style="text-align: center; flex: 1 1 48%; max-width: 48%;">
                <img src="static/images/plot_TRI_7B.png" alt="SFT-trained model" style="width: 100%; height: auto; object-fit: contain;">
                <figcaption style="margin-top: 6px;">(a)</figcaption>
              </figure>
          
              <figure style="text-align: center; flex: 1 1 48%; max-width: 48%;">
                <img src="static/images/latency_7b_1.png" alt="RL-trained model" style="width: 100%; height: auto; object-fit: contain;">
                <figcaption style="margin-top: 6px;">(b)</figcaption>
              </figure>
            </div>
          
            <figcaption style="text-align: center; margin-top: 10px;">
              Figure 2: (a) DLER achieves state-of-the-art accuracy/length trade-offs, shortening CoT by up to 70% without losing accuracy. (b) On AIME-24, DLER-R1 models enable better test-time scaling.
            </figcaption>
          </figure> -->
      </div>
    </div>
  </div>
</section>

<br>
<!-- Paper abstract -->
<!-- <section class="section hero is-light" style="padding: 1rem 1.5rem; margin-top: 0px;">

</section> -->
<!-- End paper abstract -->


<!-- Overview Section -->
<section class="section" style="padding-top: 1rem; padding-bottom: 1rem;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3" style="margin-bottom: 0.75rem;">GDPO comes to the rescue!</h2>
        <div class="content">
          <h3 class="title is-4" style="margin-bottom: 0.75rem; color:forestgreen;">Decouples group-wise normalization of each reward separately before aggregation</h3></li>
          <div class="content has-text-justified mt-4">
            <li>
              To address the fundamental limitation of GRPO, we propose <strong>Group reward-Decoupled normalization Policy Optimization (GDPO)</strong>, which performs group-wise normalization on each reward independently prior to aggregation. This contrasts with GRPO, which applies group-wise normalization directly to the summed rewards.           
            </li>
          </div>
          <figure class="image">
            <img src="static/images/GDPO_FORMULA.png" alt="CLIMB Data Filtering Architecture" style="width: 70%; height: auto;">
            <figcaption>Overview of GDPO</figcaption>
          </figure>
        </div>
        <div class="content">
          <h3 class="title is-4" style="margin-bottom: 0.75rem; color:forestgreen;">More Fine-Grained Advantage Estimates</h3></li>
          <div class="content has-text-justified mt-4">
            <li>
              By separating the normalization of each reward, GDPO alleviates the training signal collapse problem present in GRPO‚Äôs advantage estimation in Figure 4. For example, the reward combination of (0, 1) after GDPO normalization becomes (‚àí0.7071, 0 .7071) and (0, 2) becomes (‚àí1.4142 , 1.4142), which more appropriately reflects that (0, 2) should yield a stronger learning signal than (0, 1).
            </li>
          </div>
          <figure class="image">
            <img src="static/images/distinct_count.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 5: Comparison of the number of distinct advantage groups produced by GRPO and GDPO. As the number of rollouts (left) or rewards (right) grows, GDPO consistently preserve a substantially larger number of distinct advantage groups compared to GRPO. This results in advantage estimations that provide more expressive training signals</figcaption>
          </figure>
          <div class="content has-text-justified mt-4">
            <li>
              When generalized to a larger number of rollouts while keeping the number of rewards to 2, GDPO consistently produces a significantly higher count of distinct advantage groups compared to GRPO, with the gap widening as the number of rollouts increases.
              Similar trend can be observed under settings where the number of rollouts is fixed at four, but the number of rewards gradually increases.
            </li>
          </div>
        </div>
        <!-- <div class="content">
          <h3 class="title is-4" style="margin-bottom: 0.75rem; color:forestgreen;">Length Penalty Over-sparsify Training Signal</h3></li>
          <div class="content has-text-justified mt-4">
            <p>
              Applying length penalty makes early training too difficult and later stages dominated by easy samples. We adopt Dynamic Sampling, which filters out overly easy/hard prompts and extreme response lengths, yielding balanced training signals and better length control.
            </p>
          </div>
          <figure class="image">
            <img src="static/images/ds_filter_combined.png" alt="CLIMB Data Filtering Architecture">
            <figcaption>Figure 4: Left: Ratio of training prompts with all 16 rollouts receiving zero reward, including those caused by exceeding the truncation length. Around half of the prompts fall into this category early in training, weakening the signal and biasing the model toward easier prompts that model already know how to solve within the target length. Right: Ratio of training prompts with all 16 rollouts receiving reward score of one steadily increases, while average response length declines and remains markedly shorter than that for prompts whose rollouts all receive a reward of zero.</figcaption>
          </figure>
        </div> -->
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-fullwidth">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-2">Switching from GRPO to GDPO is straightforward!</h2>


        <p>
          GDPO can serve as a drop-in replacement for GRPO within
          <a href="https://github.com/volcengine/verl" target="_blank" rel="noopener noreferrer">verl</a>
          and
          <a href="https://github.com/huggingface/trl" target="_blank" rel="noopener noreferrer">TRL</a>,
          requiring only minor code changes. See <a href="https://github.com/NVlabs/GDPO" target="_blank" rel="noopener noreferrer">NVlabs/GDPO</a> for the GDPO implementation based on verl, TRL, and nemo-RL and training code to reproduce the reported results.
        </p>
        <br>
        <h3 class="title is-4" style="margin-bottom: 0.75rem; color:forestgreen;">TRL Implementation Modification to support GDPO</h3>
        <h6 class="title is-5">Original GRPO implementation based on TRL</h6>
        <div class="content has-text-left">
          <pre><code class="python">
            # line 1254 in NVlabs/GDPO/trl-GDPO/trl-0.18.0-gdpo/trl/trainer/grpo_trainer.py
            # Gather the reward per function: this part is crucial, because the rewards are normalized per group and the
            # completions may be distributed across processes
            rewards_per_func = gather(rewards_per_func)
            rewards = (rewards_per_func * self.reward_weights.to(device).unsqueeze(0)).nansum(dim=1)

            # Compute grouped-wise rewards
            mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)
            std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)
            is_std_zero = torch.isclose(std_grouped_rewards, torch.zeros_like(std_grouped_rewards))

            # Normalize the rewards to compute the advantages
            mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
            std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
            advantages = rewards - mean_grouped_rewards
            if self.scale_rewards:
                advantages = advantages / (std_grouped_rewards + 1e-4)
          </code></pre>
        </div>
        <h6 class="title is-5">GDPO implementation based on TRL</h6>
        <div class="content has-text-left">
          <pre><code class="python">
            # line 1222 in NVlabs/GDPO/trl-GDPO/trl-0.18.0-gdpo/trl/trainer/grpo_trainer.py
            # Gather the reward per function: this part is crucial, because the rewards are normalized per group and the
            # completions may be distributed across processes
            rewards_per_func = gather(rewards_per_func)
            ## Make sure every reward contain no nan value
            rewards_per_func_filter = torch.nan_to_num(rewards_per_func)

            all_reward_advantage = []
            ## Calculate the mean and std of each reward group-wise separately
            for i in range(len(self.reward_weights)):
                reward_i = rewards_per_func_filter[:,i]
                each_reward_mean_grouped = reward_i.view(-1, self.num_generations).mean(dim=1)
                each_reward_std_grouped = reward_i.view(-1, self.num_generations).std(dim=1)

                each_reward_mean_grouped = each_reward_mean_grouped.repeat_interleave(self.num_generations, dim=0)
                each_reward_std_grouped = each_reward_std_grouped.repeat_interleave(self.num_generations, dim=0)
                each_reward_advantage = reward_i - each_reward_mean_grouped
                each_reward_advantage = each_reward_advantage / (each_reward_std_grouped + 1e-4)
                all_reward_advantage.append(each_reward_advantage)

            combined_reward_advantage = torch.stack(all_reward_advantage, dim=1)
            pre_bn_advantages = (combined_reward_advantage * self.reward_weights.to(device).unsqueeze(0)).nansum(dim=1)

            ## compute batch-wise mean and std
            bn_advantages_mean = pre_bn_advantages.mean()
            bn_advantages_std = pre_bn_advantages.std()

            advantages = (pre_bn_advantages - bn_advantages_mean) / (bn_advantages_std + 1e-4)
          </code></pre>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Cluster Table Section -->

<!-- End Cluster Table Section -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
 
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

          <div class="policy-links has-text-centered">
            <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank" rel="noopener" class="policy-link">Privacy Policy</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/privacy-center/" target="_blank" rel="noopener" class="policy-link">Manage My Privacy</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/preferences/email-preferences/" target="_blank" rel="noopener" class="policy-link">Do Not Sell or Share My Data</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank" rel="noopener" class="policy-link">Terms of Service</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank" rel="noopener" class="policy-link">Accessibility</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank" rel="noopener" class="policy-link">Corporate Policies</a>
            <span class="separator">‚Ä¢</span>
            <a href="https://www.nvidia.com/en-us/contact/" target="_blank" rel="noopener" class="policy-link">Contact</a>
          </div>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
